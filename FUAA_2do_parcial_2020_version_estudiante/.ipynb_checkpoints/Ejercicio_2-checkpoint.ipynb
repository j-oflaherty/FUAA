{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> FUNDAMENTOS DE APRENDIZAJE AUTOMÁTICO <br> Y RECONOCIMIENTO DE PATRONES</center>\n",
    "## <center> 2do parcial, 2020</center>           \n",
    "\n",
    "La duración del parcial es de 3 horas. El parcial consta de 3 ejercicios, cuya suma total es de 100 puntos. El parcial es sin material y no está permitido acceder a Internet. Ante cualquier duda comuníquese con los docentes. \n",
    "\n",
    "Este notebook corresponde al ejercicio 2. Hay un notebook por ejercicio planteado.\n",
    "\n",
    "* [Ejercicio 2 - Redes Neuronales](#Ejercicio2) (40 puntos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se importan las bibliotecas que se utilizarán\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, plot_confusion_matrix, confusion_matrix\n",
    "# from numba import jit\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Ejercicio2\"></a>\n",
    "# Ejercicio 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funciones auxiliares (Ejecutar y seguir)\n",
    "def error_relativo(x, y):\n",
    "    ''' devuelve el error relativo'''\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "def calcular_gradiente_numerico_array(f, x, df, h=1e-5):\n",
    "    '''\n",
    "    Evalúa el gradiente numérico para una función que acepta un arreglo numpy y\n",
    "    devuelve un arreglo numpy.\n",
    "    '''\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h\n",
    "        pos = f(x).copy()\n",
    "        x[ix] = oldval - h\n",
    "        neg = f(x).copy()\n",
    "        x[ix] = oldval\n",
    "\n",
    "        grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n",
    "        it.iternext()\n",
    "    return grad\n",
    "\n",
    "\n",
    "def calcular_gradiente_numerico(f, x, verbose=True, h=0.00001):\n",
    "    '''\n",
    "    Evalúa el gradiente numérico de f en x\n",
    "    - f es una función que recibe un solo argumento\n",
    "    - x es el punto (numpy array) en que se evalúa el gradiente\n",
    "    '''\n",
    "    \n",
    "    # se inicializa el gradiente \n",
    "    grad = np.zeros_like(x)\n",
    "    # se define un iterador sobre todos los elementos de x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "\n",
    "        # se evalúa la función en x+h\n",
    "        ix = it.multi_index\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h # se suma h al valor original de x\n",
    "        fxph = f(x) # se evalúa f(x + h)\n",
    "        x[ix] = oldval - h\n",
    "        fxmh = f(x) # se evalúa f(x - h)\n",
    "        x[ix] = oldval # se restaura el valor original de x\n",
    "\n",
    "        # se calcula la derivada parcial con la fórmula centrada\n",
    "        grad[ix] = (fxph - fxmh) / (2 * h) \n",
    "        if verbose:\n",
    "            print(ix, grad[ix])\n",
    "        it.iternext() # step to next dimension\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción del ejercicio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejercicio se implementarán algunos de los bloques constitutivos de una red neuronal de **tres capas multiclase**. La figura muestra el diagrama de bloques para la red de tres capas que se implementará. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/red_tres_capas_multiclase.png\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se describen los bloques:\n",
    "- **Inicializar parámetros:** Inicializa los parámentros de la red. A los pesos de la capa $l$ de la red les llamaremos $W_l$, $b_l$ con $l=1,2,3$. \n",
    "- **Propagación hacia adelante:** La *propagación hacia adelante*  o *forward propagation* consiste en estimar la salida de la red a partir de la entrada. Cada nodo o capa de la red tiene un método *forward* asociado. Se proveen las implementaciones de los métodos forward asociados a los siguientes bloques:\n",
    "        - Afin\n",
    "        - Activación \n",
    "        - Afin --> Activación\n",
    "        \n",
    "- **Loss:** Calcula el valor de la función de costo a optimizar. Se implementará como función de costo:\n",
    "        - Entropía cruzada multiclase (Softmax Loss)\n",
    "\n",
    "- **Propagación hacia atrás:** Durante la *propagación hacia atrás* o *backpropagation* se calculan los gradientes necesarios para actualizar los parámetros de la red. Se implementarán métodos *backward* para los siguientes bloques:\n",
    "        - Afin\n",
    "        - Activación \n",
    "        - Afin --> Activación\n",
    "- **Update:** Es el boque encargado de actualizar los parámetros. Para ello utiliza los gradientes calculados durante la *propagación hacia atrás* y un método de optimización. Se utilizará *descenso por gradiente* como método de optimización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Bloque de Inicialización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se implementará el bloque de inicialización para el caso de una red neuronal de **tres capas** con la siguiente estructura:    \n",
    "  Afin --> Activación 1 --> Afin --> Activacion 2 --> Afin --> Softmax        \n",
    "\n",
    "### Ejercicio 2 - parte a)  \n",
    "Completar la implementación de `inicializar_pesos()`. Los pesos $W_l$ serán inicializados en valores aleatorios con distribución gaussiana de desviación estandar $\\sigma_l=1/\\sqrt{d_{l-1}}$, siendo $d_{l-1}$ el número de nodos de la capa $l-1$. Por ejemplo, para la primera capa $l=1$, la cantidad de nodos $d_{l-1}=d_0$ corresponde a la dimensión del vector de características. Los pesos correspondientes a términos de *bias* se inicializarán a cero. \n",
    "\n",
    "**Nota:** No necesario realizar una implementación genérica. Alcanza con que funcione para una red de tres capas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inicializar_pesos(dims, semilla=1):\n",
    "    \"\"\"\n",
    "    Entrada:\n",
    "        dims: lista que contiene el número de nodos de cada una de las capas. El primer elemento\n",
    "              corresponde al tamaño del vector de características y el último a la cantidad de \n",
    "              nodos en la última capa oculta.\n",
    "        semilla: semilla a utilizar para generar los valores aleatorios\n",
    "    \n",
    "    Salida:\n",
    "        parametros: diccionario de python que contiene los parámetros inicializados \n",
    "                    parametros['W' + str(l)] = ... \n",
    "                    parametros['b' + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(semilla)\n",
    "    \n",
    "    ####################################################################################\n",
    "    ###########  EMPIEZA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "    ####################################################################################\n",
    "    # Sugerencia: puede ser útil utilizar np.randn() y ajustar la desviación estándar\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ####################################################################################\n",
    "    ###########  TERMINA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "    ####################################################################################\n",
    "   \n",
    "    # Se genera el diccionario con los valores inicializados\n",
    "    parametros = {'W1': W1,\n",
    "                  'b1': b1,\n",
    "                  'W2': W2,\n",
    "                  'b2': b2,\n",
    "                  'W3': W3,\n",
    "                  'b3': b3}\n",
    "    \n",
    "    return parametros    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se testea la inicialización con pesos aleatorios\n",
    "dims = [3,6,3,1]\n",
    "parametros = inicializar_pesos(dims)\n",
    "\n",
    "W1_correcto = np.array([[ 0.93781623, -0.35319773, -0.3049401 , -0.61947872,  0.49964333, -1.32879399],\n",
    "                       [ 1.00736754, -0.43948301,  0.18419731, -0.14397405,  0.84414841, -1.18942279],\n",
    "                         [-0.18614766, -0.22173389,  0.65458209, -0.63502252, -0.09955147, -0.50683179]])\n",
    "b1_correcto = np.array([0., 0., 0., 0., 0., 0.])\n",
    "W2_correcto = np.array([[ 0.01723369,  0.23793331, -0.4493259 ],[ 0.4673315 ,  0.36807287,  0.20514245],\n",
    "                       [ 0.3677729 , -0.27913073, -0.05016972], [-0.38202627, -0.10936485,  0.21651671],\n",
    "                       [-0.28236932, -0.16197395, -0.28053708], [-0.34505376, -0.27403509, -0.0051703 ]])\n",
    "b2_correcto = np.array([0., 0., 0.])  \n",
    "W3_correcto = np.array([[-0.64507943],\n",
    "       [ 0.13533997],\n",
    "       [ 0.95828723]])\n",
    "b3_correcto = np.array([0.])\n",
    "\n",
    "# Se compara la salida con la nuestra. El error debería ser e-7 o menos.\n",
    "print('Testeando la incialización aleatoria:')\n",
    "print('Diferencia en W1: ', error_relativo(parametros['W1'], W1_correcto))\n",
    "print('Diferencia en b1: ', error_relativo(parametros['b1'], b1_correcto))\n",
    "print('Diferencia en W2: ', error_relativo(parametros['W2'], W2_correcto))\n",
    "print('Diferencia en b2: ', error_relativo(parametros['b2'], b2_correcto))\n",
    "print('Diferencia en W3: ', error_relativo(parametros['W3'], W3_correcto))\n",
    "print('Diferencia en b3: ', error_relativo(parametros['b3'], b3_correcto))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Bloques Forward\n",
    "\n",
    "Se proveen las implementaciones de los métodos *forward* de los siguientes bloques: \n",
    "\n",
    "- Bloque Afin  \n",
    "- Bloque Activación donde la activación puede ser ReLU, Sigmoide\n",
    "- Bloque Afin -> Activación  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Forward Afin\n",
    "\n",
    "La señal de entrada a la activación de la capa $\\textit{l}$ puede escribirse como:\n",
    "\n",
    "$$\n",
    "\\mathbf{s}^{(l)}=\\left( W^{(l)} \\right)^T \\mathbf{x}^{(l-1)}+ \\mathbf{b}^{(l)}   \\tag{1}\n",
    "$$\n",
    "\n",
    "donde $\\mathbf{s}^{(l)}$ y $\\mathbf{b}^{(l)}$ son vectores de tamaño $d^{(l)}$, $\\mathbf{x}^{(l-1)}$  es un vector de tamaño $d^{(l-1)}$ y $W^{(l)}$ es una matriz de tamaño $d^{(l-1)} \\times d^{(l)}$.\n",
    "\n",
    "La ecuación (1) es válida cuando la entrada a la capa es un único vector $\\mathbf{x}^{(l-1)}$. En la práctica es más habitual procesar un $\\textit{batch}$ de vectores de entrada a la vez, por lo tanto es deseable contar con una expresión que genere la salida para todos los vectores de entrada a la vez. Al evitar la utilización de un bloque $\\textit{for}$ que itere por cada una de las muestras del $\\textit{batch}$ se mejora la eficiencia de la implementación.   \n",
    "\n",
    "\n",
    "La versión de la ecuación (1) que actúa sobre un conjunto de muestras a la vez es la siguiente:\n",
    "\n",
    "$$\n",
    "S^{(l)} = X^{(l-1)}W^{(l)} +\\mathbf{b}^{(l)}\\tag{2}\n",
    "$$\n",
    "\n",
    "donde $X^{[0]} = X$, siendo X una matriz que contiene un vector de características en cada fila."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def afin_forward(X, W, b):\n",
    "    \"\"\"\n",
    "    Implementa la propagación hacia adelante en una capa afin.\n",
    "\n",
    "    Entrada:\n",
    "        X: arreglo de tamaño (N, dim capa anterior) que en cada fila contiene un vector de\n",
    "           activaciones de la capa anterior (o datos de entrada)\n",
    "        W: arreglo de pesos de tamaño (dim de capa anterior, dim de capa actual) \n",
    "        b: vector de bias de tamaño (dim de la capa actual,)\n",
    "\n",
    "    Salida:\n",
    "        S: arreglo de tamaño (N, dim de capa actual) que contiene\n",
    "           los scores o señal de entrada a la activación  \n",
    "        cache: (X, W, b) tupla que contiene X, W y b. \n",
    "               Son almacenados para calcular el paso backward eficientemente\n",
    "    \"\"\"\n",
    "\n",
    "    S = np.dot(X, W) + b\n",
    "    \n",
    "    assert(S.shape == (X.shape[0], W.shape[1] ))\n",
    "    cache = (X, W, b)\n",
    "    \n",
    "    return S, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Funciones de activación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se proveen las implementaciones de las siguientes funciones de activación:\n",
    "\n",
    "- **Sigmoide**: $\\sigma(S) = \\sigma(X W  + \\mathbf{b}) = \\frac{1}{ 1 + e^{-(X W  + b)}}$. Esta función devuelve, además de la activación resultante, la variable cache que contiene la señal `S` que dio lugar a la activación (se utiilza luego durante la propagación hacia atrás).\n",
    "\n",
    "``` python\n",
    "X, cache = sigmoid(S)\n",
    "```\n",
    "\n",
    "\n",
    "- **Rectified Linear Unit**:  $ReLU(S) = \\max(0, S)$.  Al igual que en el caso de la activación sigmoide, esta función devuelve además de la activación resultante, la variable cache que contiene la señal `S` que dio lugar a la activación (se utiilza luego durante la propagación hacia atrás).\n",
    "\n",
    "``` python\n",
    "X, cache = relu(S)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoide(S):\n",
    "    \"\"\"\n",
    "    Implementa la activación sigmoide\n",
    "    \n",
    "    Entrada:\n",
    "        S: arreglo numpy que contiene las entradas a la activación. \n",
    "           Las dimensiones de entrada no están definidas.\n",
    "    \n",
    "    Salida:\n",
    "        X: arreglo del mismo tamaño que S que contiene la salida de sigmoid(S) \n",
    "    cache: devuelve S para utilizar durante la propagación hacia atrás\n",
    "    \"\"\"\n",
    "\n",
    "    X = 1/(1+np.exp(-S))\n",
    "    cache = S\n",
    "\n",
    "    assert X.shape == S.shape, 'La entrada y la salida deben ser del mismo tamaño'\n",
    "    return X, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(S):\n",
    "    '''\n",
    "    Implementa la activación relu\n",
    "    \n",
    "    Entrada:\n",
    "        S: arreglo numpy que contiene las entradas a la activación. \n",
    "           Las dimensiones de entrada no están definidas.\n",
    "    \n",
    "    Salida:\n",
    "        X: arreglo del mismo tamaño que S que contiene la salida de relu(S) \n",
    "    cache: devuelve S para utilizar durante la propagación hacia atrás\n",
    "    '''\n",
    "    \n",
    "    X = np.maximum(0,S)\n",
    "    \n",
    "    assert(X.shape == S.shape)\n",
    "    cache = S \n",
    "        \n",
    "    return X, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Aplicación conjunta de capa afin y activación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se provee la implementación de la propagación hacia adelante de una capa *Afin->Activacion*. El método `afin_activacion_forward()` implementa la operación:\n",
    "\n",
    "$$\n",
    "X^{[l]} = \\theta(S^{(l)}) = \\theta(X^{(l-1)}W^{(l)} + \\mathbf{b}^{(l)})  \\tag{3}\n",
    "$$\n",
    "\n",
    "donde la activación $\\theta(\\cdot)$ será alguna de las implementadas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def afin_activacion_forward(X_prev, W, b, activacion):\n",
    "    \"\"\"\n",
    "    Implementa la propagación hacia adelante para una capa Afin->Activación \n",
    "    Entrada:\n",
    "        X_prev: arreglo de tamaño (N, dim capa anterior) que contiene la \n",
    "                activación de la capa anterior (o datos de entrada):          \n",
    "        W: matriz de pesos de tamaño (dim de capa anterior, dim de capa actual)  \n",
    "        b: vector de bias de tamaño (dim de la capa actual)\n",
    "        activacion: la activacion a utilizar en esta capa se indica con uno de los \n",
    "                    siguientes strings: 'sigmoide', 'tanh' o 'relu'\n",
    "\n",
    "    Salida:\n",
    "        X: arreglo de tamaño (N, dim de capa actual) que contiene la salida \n",
    "           de la función de activación  \n",
    "    cache: tupla que contiene \"cache_afin\" y \"cache_activacion\".\n",
    "           Se almacenan para calcular la propagación hacia atrás eficientemente\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    S, cache_afin = afin_forward(X_prev,W,b)\n",
    "    \n",
    "    if activacion == 'sigmoide':    \n",
    "        \n",
    "        X, cache_activacion = sigmoide(S)\n",
    "    \n",
    "    elif activacion == 'relu':\n",
    "        \n",
    "        X, cache_activacion = relu(S)\n",
    "    \n",
    "    assert (X.shape == (X_prev.shape[0], W.shape[1]))\n",
    "    cache = (cache_afin, cache_activacion)\n",
    "\n",
    "    return X, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Función de costo\n",
    "\n",
    "En esta sección se implementará el bloque `softmax_loss()`, muy utilizado para problemas de clasificación multiclase. Dentro del mismo se realizan dos operaciones: \n",
    "\n",
    "1. Se aplica la no linealidad softmax sobre los scores entrantes\n",
    "\n",
    "$$\n",
    "\\mathbf{x}^{(L)} = softmax(\\mathbf{s}^{(L)}) = \\sigma_s({\\mathbf{s}^{(L)}})  = \\frac{\\exp(\\mathbf{s}^{(L)})}{\\sum_{k=1}^{K} \\exp(s^{(L)}_k)}  \\tag{4}\n",
    "$$\n",
    "\n",
    "Observar que esta operación da como resultado un vector con valores entre cero y uno, cuya suma es uno. Es habitual que cada componente de la salida del softmax se interprete como la probabilidad de pertenencer a una clase.\n",
    "\n",
    "2. Se utiliza como función de costo la entropía cruzada multiclase. Cuando se tienen etiquetas codificadas con vectores *one-hot*, la salida ideal en el paso anterior para una muestra $\\mathbf{x}_i$ perteneciente a la clase $y_i$,  es un vector que vale uno en la componente $y_i$ y cero en las demás. La entropía cruzada es una función de costo que puede ser utilizada para cuantificar cuanto se aparta la salida del softmax de la salida ideal. Si se llama  $\\mathbf{x}^{(L)}$ al vector de probabilidades brindado por el softmax  y $\\mathbf{y^{oh}}$ al vector etiqueta codificado mediante *one-hot*, la entropía cruzada se define como\n",
    "\n",
    "$$\n",
    "H(\\mathbf{\\mathbf{x}^{(L)}}, \\mathbf{y^{oh}})= - \\sum\\limits_{k = 0}^{K-1}  y^{oh}_k\\log x^{(L)}_k  = - \\log x^{(L)}_{y_i}  \\tag{5}\n",
    "$$\n",
    "\n",
    "donde K es el número de clases. Notar que en la última igualdad se utilizó que el único término de la sumatoria que sobrevive es el $k$ para el cual $y^{oh}_k$ es distinto de cero, es decir, la componente de la etiqueta $y_i$. Si llamamos $L_i$ al costo asociado a la muestra $i$, entonces se tiene que:\n",
    "\n",
    "$$\n",
    "L_i = - \\log x^{(L)}_{y_i} = - \\log \\left( \\sigma_s \\left(\\mathbf{s}_i^{(L)} \\right) \\right)_{y_i}  \\tag{6}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2 - parte b)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argumentar que la derivada del costo $L_i$ respecto a la *l-ésima* componente de $\\mathbf{s}^{(L)}$ (entrada al bloque `softmax_loss`) es:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L_i}{\\partial s^{(L)}_l} = \\begin{cases}\n",
    "                                                \\sigma_s({s^{(L)}_l})  \\qquad  \\qquad \\forall l \\neq y_i  \\\\\n",
    "                                                \\sigma_s({s^{(L)}_l}) - 1  \\qquad  \\text{  si } l=y_i\n",
    "                                     \\end{cases} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sugerencia:** Utilizar que la derivada parcial de la *j-ésima* componente de la salida del softmax respecto a la *l-esima* entrada se calcula de la siguiente manera:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\frac{\\partial }{\\partial s_l} \\sigma_s({s_j}) & = \\frac{\\partial }{\\partial s_l} \\frac{\\exp(s_j)}{\\sum_{k=0}^{K-1} \\exp(s_k)} \\\\\n",
    "& = \\frac{ \\frac{\\partial \\exp(s_j)}{\\partial s_l} \\sum_{k=0}^{K-1} \\exp(s_k) - \\exp(s_j) \\frac{\\partial \\sum_{k=0}^{K-1} \\exp(s_k)  }{\\partial s_l}  } { \\left( \\sum_{k=0}^{K-1} \\exp(s_k) \\right) ^2} \\\\\n",
    "& = \\frac{ \\exp(s_j) \\delta_{jl} \\sum_{k=0}^{K-1} \\exp(s_k)  - \\exp(s_j) \\exp(s_l)} { \\left( \\sum_{k=0}^{K-1} \\exp(s_k) \\right) ^2}  \\\\\n",
    "& = \\sigma_s({s_j}) \\delta_{jl} - \\sigma_s({s_j})\\sigma_s({s_l})  \\\\\n",
    "& = \\sigma_s({s_j}) \\left( \\delta_{jl} - \\sigma_s({s_l}) \\right) \n",
    "\\end{align}\n",
    "\n",
    "donde $\\delta_{jl}$ vale uno cuando $j$ es igual a $l$ y cero en caso contrario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resupuesta:**    \n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2 - parte c)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementar el bloque `softmax_loss()`. El costo asociado a un conjunto de $N$ muestras se define como:\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{N}\\sum_{i=0}^{N-1}L_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANTE:** En esta parte dejar comentado el *@jit* que aparece sobre el encabezado de la función. La sentencia es un decorador que cuando está activa indica que la función será compilada para su posterior ejecución. \n",
    "Mas adelante se le pedirá entrenar la red. Si el entrenamiento le demora mucho, pruebe: i) descomentar el @jit ii) ejecutar nuevamente la celda que implementa el `softmax_loss` iii) entrenar nuevamente la red. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@jit\n",
    "def softmax_loss(S, y):\n",
    "    '''\n",
    "    Implementa softmax sobre los scores para generar probabilidades de pertenencia a cada clase y luego\n",
    "    utiliza la entropía cruzada como función de costo\n",
    "    \n",
    "    Entrada:\n",
    "       S: arreglo de dimensión (N, K) que contiene los scores que ingresan a la capa softmax\n",
    "       y: arreglo de tamaño (N,) que contiene las etiquetas\n",
    "    Salida:\n",
    "       costo: escalar con el costo correspondiente a la entropía cruzada\n",
    "       dS: Gradiente del costo respecto a los scores\n",
    "    '''\n",
    "    N = S.shape[0]\n",
    "    K = S.shape[1]\n",
    "    \n",
    "    ####################################################################################\n",
    "    ###########  EMPIEZA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "    ####################################################################################\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ####################################################################################\n",
    "    ###########  TERMINA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "    ####################################################################################    \n",
    "       \n",
    "    return costo, dS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de afin_backward\n",
    "np.random.seed(43)\n",
    "Np = 20\n",
    "n_classes = 9\n",
    "S_ = np.random.randn(Np, n_classes)\n",
    "y_ = np.random.randint(n_classes, size=Np)\n",
    "\n",
    "dS_num = calcular_gradiente_numerico(lambda S: softmax_loss(S, y_)[0], S_, verbose = False)\n",
    "\n",
    "costo, dS = softmax_loss(S_, y_)\n",
    "\n",
    "assert np.isscalar(costo), 'El costo debe ser un escalar'\n",
    "assert np.allclose(costo, 2.626917569953734), 'Revisar el cálculo del costo'\n",
    "# El error debería ser del orden de e-8 o menos\n",
    "print('Testing softmax_loss():')\n",
    "print('dS error: ', error_relativo(dS_num, dS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Propagación hacia atrás"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se implementará la versión *backward* de cada una de las funciones *forward* implementadas anteriormente. A saber:\n",
    "- AFIN backward\n",
    "- ACTIVACION backward \n",
    "- AFIN -> ACTIVACION backward donde ACTIVACION puede ser *ReLU* o *sigmoide* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - Afin backward\n",
    "\n",
    "Durante la propagación hacia adelante en la capa $l$ (sin considerar la activación) se calcula para una muestra: \n",
    "\n",
    "$$\n",
    "\\mathbf{s}^{(l)}=\\left( W^{(l)} \\right)^T \\mathbf{x}^{(l-1)}+ \\mathbf{b}^{(l)}   \\tag{1}\n",
    "$$\n",
    "\n",
    "Si se llama $e_n$ al costo debido a la muesta $n$ y se asume conocido el *vector de sensibilidad* $\\delta^{(l)}=\\frac{\\partial e_n}{\\partial \\mathbf{s}^{(l)}}$, en el teórico del curso se vio que \n",
    "\n",
    "$$\n",
    "\\frac{\\partial{e_n}}{\\partial{W^{(l)}}}=\\mathbf{x}^{(l-1)} \\left( \\delta^{(l)} \\right)^T\n",
    "$$\n",
    "\n",
    "Análogamente a como se hizo en el caso de la propagación hacia adelante, si se considera la contribución al error de un conjunto de muestras a la vez la ecuación se puede escribir en forma vectorizada como:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{E}}{\\partial{W^{(l)}}}= dW^{(l)} = \\left( X^{(l-1)}\\right)^ T dS^{(l)}   \\tag{5}\n",
    "$$\n",
    "\n",
    "donde $dS^{(l)}$ es una matríz de tamaño $N\\times d^{(l)}$ que en cada fila contiene el vector de sensibilidad $\\delta^{(l)}_n$ correspondiente a una de las muestras.\n",
    "\n",
    "Las derivadas respecto al vector de bias $\\mathbf{b}^{(l)}$ se calculan de forma similar (puede pensarse como un caso particular en que $X^{(l-1)}$ es un vector columna de unos) por lo que\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{E}}{\\partial{\\mathbf{b}^{(l)}}}= d\\mathbf{b}^{(l)} =\\mathbb{1} ^ T dS^{(l)}  \\tag{6}\n",
    "$$\n",
    "\n",
    "Finalmente se calcula la influencia de cada una de las características en el error. Considerando primero el caso de una muestra, se tiene que:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{e_n}}{\\partial{\\mathbf{x}^{(l-1)}}} = W^{(l)} \\delta^{(l)}\n",
    "$$\n",
    "\n",
    "que en forma vectorizada puede escribirse como:\n",
    "\n",
    "$$ \n",
    " \\frac{\\partial E }{\\partial X^{(l-1)}} = dX^{(l-1)} = dS^{(l)} \\left( W^{(l) }\\right)^T \\tag{7}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2 - parte d)  \n",
    "Implementar el método `afin_backward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def afin_backward(dS, cache):\n",
    "    \"\"\"\n",
    "    Implementa la propagación hacia atrás para una capa l (sin considerar la activación)\n",
    "\n",
    "    Entrada:\n",
    "        dS: Gradiente de la función de costo con respecto a la salida de la capa actual \n",
    "            (sin considerar la activación)\n",
    "        cache: tupla de valores (X_prev, W, b) calculados durante la propagación hacia adelante\n",
    "               de la capa actual\n",
    "\n",
    "    Salida:\n",
    "        dX_prev: Gradiente de la función de costo con respecto a la activación de la capa anterior (l-1), \n",
    "                 tiene el mismo tamaño que X_prev\n",
    "        dW: Gradiente de la función de costo con respecto a W (de la capa actual l), \n",
    "            tiene el mismo tamaño que W\n",
    "        db: Gradiente de la función de costo con respecto a b (de la capa actual l), \n",
    "            tiene el mismo tamaño que b\n",
    "    \"\"\"\n",
    "    X_prev, W, b = cache\n",
    "    N = X_prev.shape[0]\n",
    "\n",
    "    ####################################################################################\n",
    "    ###########  EMPIEZA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "    ####################################################################################\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ####################################################################################\n",
    "    ###########  TERMINA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "    ####################################################################################\n",
    "\n",
    "    assert (dX_prev.shape == X_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dX_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de afin_backward\n",
    "np.random.seed(43)\n",
    "x = np.random.randn(10, 6)\n",
    "w = np.random.randn(6, 5)\n",
    "b = np.random.randn(5)\n",
    "dout = np.random.randn(10, 5)\n",
    "\n",
    "#dx_num = calcular_gradiente_numerico_array(lambda x: afin_forward(x, w, b)[0], x, dout)\n",
    "dx_num = calcular_gradiente_numerico_array(lambda xx: afin_forward(xx, w, b)[0], x, dout)\n",
    "\n",
    "dw_num = calcular_gradiente_numerico_array(lambda ww: afin_forward(x, ww, b)[0], w, dout)\n",
    "db_num = calcular_gradiente_numerico_array(lambda bb: afin_forward(x, w, bb)[0], b, dout)\n",
    "\n",
    "_, cache = afin_forward(x, w, b)\n",
    "dx, dw, db = afin_backward(dout, cache)\n",
    "\n",
    "# El error debería ser del orden de e-9 o menos\n",
    "print('Testing afin_backward():')\n",
    "print('dx error: ', error_relativo(dx_num, dx))\n",
    "print('dw error: ', error_relativo(dw_num, dw))\n",
    "print('db error: ', error_relativo(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Activación backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si  $\\theta(\\cdot)$ es la función de activación, entonces su función *backward* se calcula \n",
    "\n",
    "$$\n",
    "dS^{(l)} = dX^{(l)} * \\theta'(S^{(l)})   \\tag{8}\n",
    "$$.  \n",
    "\n",
    "donde $\\theta'(\\cdot)$ debe ser calculado para cada caso. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2 - parte e)  \n",
    "Implementar los métodos *backward* de cada una de las funciones de activación implementadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoide_backward(dX, cache):\n",
    "    \"\"\"\n",
    "    Implementa la propagación hacia atrás de una activación Sigmoide.\n",
    "\n",
    "    Entrada:\n",
    "        dX: gradiente de la función de costo respecto a la salida de la capa sigmoide,\n",
    "              el tamaño del arreglo no está definido\n",
    "        cache: 'S' valor almacenado durante la propagación hacia adelante\n",
    "\n",
    "    Returns:\n",
    "    dS -- Gradiente del costo respecto a S\n",
    "    \"\"\"\n",
    "    \n",
    "    S = cache\n",
    "    \n",
    "    ####################################################################################\n",
    "    ###########  EMPIEZA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "    ####################################################################################\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ####################################################################################\n",
    "    ###########  TERMINA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "    ####################################################################################\n",
    "    \n",
    "    assert (dS.shape == S.shape), 'dS y S no tienen el mismo tamaño'\n",
    "    assert (dX.shape == S.shape), 'dX y S no tienen el mismo tamaño'\n",
    "    \n",
    "    return dS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de sigmoid_backward\n",
    "np.random.seed(231)\n",
    "S = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*S.shape)\n",
    "\n",
    "dS_num = calcular_gradiente_numerico_array(lambda S: sigmoide(S)[0], S, dout)\n",
    "\n",
    "_, cache = sigmoide(S)\n",
    "dS = sigmoide_backward(dout, cache)\n",
    "\n",
    "# El error debería ser del orden de e-10 o menos\n",
    "print('Testing sigmoide_backward():')\n",
    "print('dS error: ', error_relativo(dS_num, dS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dX, cache):\n",
    "    \"\"\"\n",
    "    Implementa la propagación hacia atrás de una activación ReLu.\n",
    "\n",
    "    Entrada:\n",
    "        dX: gradiente de la función de costo respecto a la salida de la capa relu,\n",
    "              el tamaño del arreglo no está definido\n",
    "        cache: 'S' valor almacenado durante la propagación hacia adelante\n",
    "\n",
    "    Returns:\n",
    "    dS -- Gradiene del costo respecto a S\n",
    "    \"\"\"\n",
    "    \n",
    "    S = cache\n",
    "    \n",
    "    ####################################################################################\n",
    "    ###########  EMPIEZA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "    ####################################################################################\n",
    "    \n",
    "    # se inicializa dS con las dimensiones adecuadas.\n",
    "    \n",
    "    # Cuando s <= 0, dS es igual a cero. \n",
    "    \n",
    "    \n",
    "    ####################################################################################\n",
    "    ###########  TERMINA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "    ####################################################################################\n",
    "    \n",
    "    assert (dS.shape == S.shape)\n",
    "    \n",
    "    return dS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de relu_backward\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dS_num = calcular_gradiente_numerico_array(lambda x: relu(x)[0], x, dout)\n",
    "\n",
    "_, cache = relu(x)\n",
    "dS = relu_backward(dout, cache)\n",
    "\n",
    "# El error debería ser del orden de e-12\n",
    "print('Testing relu_backward():')\n",
    "print('dS error: ', error_relativo(dS_num, dS))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Afin --> Activacion backward\n",
    "\n",
    "A continuación se implementará la función que realiza la propagación hacia atrás del la capa *Afin-->Activacion*. \n",
    "\n",
    "### Ejercicio 2 - parte f)  \n",
    "Implementar la función `afin_activacion_backward()`. Para ello utilizar las funciones implementadas anteriormente: `afin_backward` y la `activacion_backward` que corresponda. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def afin_activacion_backward(dX, cache, activacion):\n",
    "    '''\n",
    "    Implementar la propagación hacia atrás para la capa Afin->Activacion.\n",
    "    \n",
    "    Entradas:\n",
    "        dX: gradiente del costo respecto a la salida de la capa actual \n",
    "        cache: tupla con los valores(cache_afin, cache_activacion) \n",
    "        activacion: la activación a utilizar en esta capa, puede ser 'sigmoide' o 'relu'\n",
    "    Salidas:\n",
    "        dX_prev: Gradiente del costo con respecto a la activación de la capa anterior(l-1), \n",
    "                 tiene las mismas dimensiones que X_prev\n",
    "        dW -- Gradiente del costo con respecto a W (de la capa actual l), \n",
    "              tiene las mismas dimensiones que W\n",
    "        db -- Gradiente del costo con respecto a b (de la capa actual l), \n",
    "              tiene las mismas dimensiones que b\n",
    "    '''\n",
    "    cache_afin, cache_activacion = cache\n",
    "    \n",
    "    ####################################################################################\n",
    "    ###########  EMPIEZA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "    ####################################################################################\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ####################################################################################\n",
    "    ###########  TERMINA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "    ####################################################################################\n",
    "    \n",
    "    return dX_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "x = np.random.randn(2, 12)\n",
    "w = np.random.randn(12, 10)\n",
    "b = np.random.randn(10)\n",
    "dout = np.random.randn(2, 10)\n",
    "\n",
    "activaciones = ['relu', 'sigmoide']\n",
    "\n",
    "for activacion in activaciones:\n",
    "    out, cache = afin_activacion_forward(x, w, b, activacion)\n",
    "    dx, dw, db = afin_activacion_backward(dout, cache, activacion)\n",
    "\n",
    "    dx_num = calcular_gradiente_numerico_array(lambda x: afin_activacion_forward(x, w, b, activacion)[0], x, dout)\n",
    "    dw_num = calcular_gradiente_numerico_array(lambda w: afin_activacion_forward(x, w, b, activacion)[0], w, dout)\n",
    "    db_num = calcular_gradiente_numerico_array(lambda b: afin_activacion_forward(x, w, b, activacion)[0], b, dout)\n",
    "\n",
    "    # Los errores deberían ser del orden de e-9 o menos\n",
    "    print('Testing afin_' + activacion + '_forward y afin_' + activacion + '_backward:')\n",
    "    print('dx error: ', error_relativo(dx_num, dx))\n",
    "    print('dw error: ', error_relativo(dw_num, dw))\n",
    "    print('db error: ', error_relativo(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  5 - Actualización de los parámetros\n",
    "\n",
    "En esta sección se actualizarán los parámetros del modelo mediante el método de *descenso por gradiente*:\n",
    "\n",
    "$$ W^{(l)} = W^{(l)} -\\eta \\text{ } dW^{(l)} \\tag{9}$$\n",
    "$$ \\mathbf{b}^{(l)} = \\mathbf{b}^{(l)} -\\eta \\text{ } \\mathbf{db}^{(l)} \\tag{10}$$\n",
    "\n",
    "donde $\\eta$ es el *learning rate*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2 - parte g)  \n",
    "Implementar `actualizar_parametros()` para actualizar los parámetros usando *descenso por gradiente*. Luego de actualizar los parámetros, almacenarlos en el diccionario de parámetros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actualizar_parametros(parametros, gradientes, learning_rate):\n",
    "    \"\"\"\n",
    "    Se actualizan los parámetros utilizando descenso por gradiente. Si bien en este notebook se trabaja \n",
    "    con una red de tres capas, el método se implementa en forma genérica.\n",
    "    \n",
    "    Entrada:\n",
    "        parametros: diccionario de python que contiene los parámetros \n",
    "                        parametros[\"W\" + str(l)] = ... \n",
    "                        parametros[\"b\" + str(l)] = ...\n",
    "        gradientes: diccionario de python que contiene los gradientes \n",
    "                    (las salidas de los métodos backward)\n",
    "                        gradientes[\"W\" + str(l)] = ... \n",
    "                        gradientes[\"b\" + str(l)] = ...\n",
    "    \n",
    "    Salida:\n",
    "        parametros: diccionario de python que contiene los parámetros actualizados \n",
    "                    parametros[\"W\" + str(l)] = ... \n",
    "                    parametros[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parametros) // 2 # número de capas en la red neuronal\n",
    "    \n",
    "    # Se actualiza cada uno de los parámetros. En el caso de una red profunda de L capas\n",
    "    # se hace con un loop que va recorriendo cada parámetro\n",
    "    for l in range(1,L+1):\n",
    "        \n",
    "        ####################################################################################\n",
    "        ###########  EMPIEZA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "        ####################################################################################\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        ####################################################################################\n",
    "        ###########  TERMINA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "        ####################################################################################\n",
    "    \n",
    "    return parametros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de dígitos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección se utilizarán los bloques implementados para clasificar dígitos. A continuación se levantan los dígitos y se los divide en conjunto de entrenamiento y test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "X, y = load_digits(return_X_y=True)\n",
    "N, d = X.shape\n",
    "N_train = 1000\n",
    "indices = np.random.permutation(N)\n",
    "X_train, y_train = X[indices[:N_train]], y[indices[:N_train]]\n",
    "X_val, y_val = X[indices[N_train:]], y[indices[N_train:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(1, 10, figsize=(12,30))\n",
    "for ind, ax in enumerate(axes):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(X[ind].reshape(8,8), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax.set_title('%i' % y[ind])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para clasificar los datos sintéticos se utilizará una red de **tres capas** con la siguiente arquitectura:   \n",
    "\n",
    "*Afin->Relu->Afin->Relu-->Afin-->Softmax* \n",
    "\n",
    "### Ejercicio 2 - parte h)  \n",
    "Completar la implementación del método `red_tres_capas()` utilizando los métodos *forward* y *backward* adecuados para dicha arquitectura. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def red_tres_capas(X, Y, dims_capas, num_iter = 1000, learning_rate = 1,\n",
    "                    mostrar_costo=False, semilla=100):\n",
    "    \"\"\"\n",
    "    Implementa una red neuronal de tres capas: Afin->Relu->Afin->Relu->Afin->Softmax.\n",
    "    \n",
    "    Entrada:\n",
    "        X: datos de entrada, de tamaño (N, d_0)\n",
    "        Y: etiquetas (1 para la clase positiva y 0 para la negativa), de tamaño (N,1)\n",
    "        dims_capas: dimensiones de las capas(d_0, d_1, d_2, d_3)\n",
    "        num_iter: número de iteraciones del loop de optimización\n",
    "        learning_rate: learning rate utilizado para la actualización mediante descenso por gradiente\n",
    "        mostrar_costo: Si vale True, se muestra el costo cada 100 iteraciones \n",
    "        semilla: semilla utilizada para la generación de números aleatorios\n",
    "    Salida:\n",
    "        parametros: un diccionario de python que contiene W1, W2, W3, b1, b2 and b3\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(semilla)\n",
    "    gradientes = {} # se inicializa el diccionario que almacena los gradientes\n",
    "    costos = []     # lista que almacena el costo\n",
    "    N = X.shape[0]  # número de muestras\n",
    "    \n",
    "    # Se inicializan los parámetros del diccionario llamando a una de las \n",
    "    # funciones previamente implementadas\n",
    "    parametros = inicializar_pesos(dims_capas, semilla=semilla)\n",
    "     \n",
    "    # Se obtienen W1, b1, W2 y b2 del diccionario de parámetros.\n",
    "    W1 = parametros[\"W1\"]\n",
    "    b1 = parametros[\"b1\"]\n",
    "    W2 = parametros[\"W2\"]\n",
    "    b2 = parametros[\"b2\"]\n",
    "    W3 = parametros[\"W3\"]\n",
    "    b3 = parametros[\"b3\"]\n",
    "    # Loop (descenso por gradiente)\n",
    "\n",
    "    for i in range(0, num_iter):\n",
    "\n",
    "        ####################################################################################\n",
    "        ###########  EMPIEZA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "        ####################################################################################\n",
    "        \n",
    "        # Propagación hacia adelante: Afin -> Relu -> Afin -> Relu -> Afin -> Softmax. \n",
    "        # Entradas: \"X, W1, b1\". Salidas: \"X1, cache1, X2, cache2, X3, cache3\".\n",
    "        \n",
    "       \n",
    "        \n",
    "        # Se calcula el costo y se inicia la propagación hacia atrás\n",
    "        \n",
    "        # Propagación hacia atrás. \n",
    "        # Entradas: \"dX3, cache3, cache2, cache1\". \n",
    "        # Salidas: \"dX2, dW3, db3, dX1, dW2, db2, dW1, db1, dX0 (no utilizado)\".\n",
    "\n",
    "        \n",
    "        \n",
    "        # Se almacenan los gradientes recientemente calculados en el diccionario \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Se actualizan los parámetros\n",
    "\n",
    "        \n",
    "        # Se obtienen los nuevos W1, b1, W2, b2, W3 y b3 del diccionario de parámetros.        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ####################################################################################\n",
    "        ###########  TERMINA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "        ####################################################################################\n",
    "        # Se muestra la evolución del costo cada 100 iteraciones\n",
    "        if mostrar_costo and i % 500 == 0:\n",
    "            print(\"Costo luego de iteracion {}: {}\".format(i, np.squeeze(costo)))\n",
    "\n",
    "        if mostrar_costo and i % 500 == 0:\n",
    "            costos.append(costo)\n",
    "    \n",
    "    # se muestra el costo\n",
    "    if mostrar_costo:    \n",
    "        plt.plot(np.squeeze(costos))\n",
    "        plt.ylabel('costo')\n",
    "        plt.xlabel('iteraciones (sobre 100)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "    \n",
    "    return parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Se definen las constantes que determinan la arquitectura de la red ####\n",
    "d_0 = X_train.shape[1]   \n",
    "d_1 = 6\n",
    "d_2 = 15\n",
    "d_3 = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se entrena la red, con los parámetros por defecto el costo debería ser alrededor de 10.17 en la iteración 0 y \n",
    "# menor a 0.15 en la 1000\n",
    "# ATENCION: En caso de que le demore mucho el entrenamiento pruebe descomentar \n",
    "# el decorador @jit del método softmax_loss() e inténtelo nuevamente.\n",
    "parametros_red_3capas = red_tres_capas(X_train, y_train, dims_capas = [d_0, d_1, d_2, d_3], \n",
    "                                    learning_rate = 1e-2, num_iter = 5000, mostrar_costo=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2 - parte i)  \n",
    "Completar la implementación del método `clasificar_digito()` y utilizarla para clasificar los dígitos con los conjuntos de entrenamiento y test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clasificar_digito(X, parametros):\n",
    "    \"\"\"\n",
    "    Esta función clasifica los dígitos utilizando una red neuronal de tres capas. \n",
    "    \n",
    "    Entrada:\n",
    "        X: matriz de tamaño Nx64 que en cada fila contiene un digito\n",
    "        parametros: parametros del modelo ya entrenado\n",
    "    \n",
    "    Salida:\n",
    "        p : vector de tamaño N que contiene las predicciones realizadas\n",
    "    \"\"\"\n",
    "    \n",
    "    # Se obtienen W1, b1, W2, b2, W3 y b3 del diccionario de parámetros.\n",
    "    W1 = parametros[\"W1\"]\n",
    "    b1 = parametros[\"b1\"]\n",
    "    W2 = parametros[\"W2\"]\n",
    "    b2 = parametros[\"b2\"]\n",
    "    W3 = parametros[\"W3\"]\n",
    "    b3 = parametros[\"b3\"]\n",
    "    \n",
    "    N = X.shape[0]\n",
    "    p = np.zeros((N,1))\n",
    "    \n",
    "    ####################################################################################\n",
    "    ###########  EMPIEZA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "    ####################################################################################\n",
    "    \n",
    "    # Se hace la propagación hacia adelante de los datos de entrada X. Tener en cuenta que la\n",
    "    # arquitectura utilizada en la red fue Afin-->Relu-->Afin-->Relu-->Afin-->Softmax\n",
    "    # ~ 3 lineas de codigo\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Se obtienen las predicciones. \n",
    "    # ~ 1 linea de codigo\n",
    "    # p =\n",
    "\n",
    "    \n",
    "    ####################################################################################\n",
    "    ###########  TERMINA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "    ####################################################################################\n",
    "\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicciones_train = clasificar_digito(X_train, parametros_red_3capas)\n",
    "predicciones_val = clasificar_digito(X_val, parametros_red_3capas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2 - parte j)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcule el porcentaje de aciertos en entrenamiento y validación y verifique en ambos casos es superior al 90\\%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "###########  EMPIEZA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "####################################################################################\n",
    "    \n",
    "# porcentaje_aciertos_train =\n",
    "\n",
    "# porcentaje_aciertos_val = \n",
    "\n",
    "\n",
    "####################################################################################\n",
    "###########  TERMINA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "####################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('El porcentaje de aciertos con los datos de entrenamiento es %f' % porcentaje_aciertos_train)\n",
    "print('El porcentaje de aciertos con los datos de validación es %f' % porcentaje_aciertos_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2 - parte k)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecute la siguiente celda y explique en qué consisten las medidas *precision* y *recall*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification report for classifier:\\n%s\\n\"\n",
    "      % (classification_report(y_val, predicciones_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Respuesta:**   \n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
