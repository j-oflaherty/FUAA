{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Ejercicio1\"></a>\n",
    "# Ejercicio 2: Regresión Logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En un problema de dos clases, el modelo de regresión logística asume que la probabilidad a posteriori de pertenencia a la clase positiva puede ser escrita de la siguiente forma:\n",
    "\n",
    "$$\n",
    "P(y=1|\\mathbf{x}_n;\\mathbf{w})= \\theta \\left( \\mathbf{w}^T\\mathbf{x}_n \\right)= \\frac{1}{1+\\exp\\left(-\\left(\\mathbf{w}^T\\mathbf{x}_n\\right)\\right)}\n",
    "$$\n",
    "\n",
    "donde $\\mathbf{x_n}=\\left( 1, x_{n1},x_{n2},...,x_{nd}\\right)$ es el n-ésimo vector de característcas expresado en coordenadas homogéneas y $d$ es el número de característcas.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ej3 a)** Mostrar que $P(y=y_n|\\mathbf{x}_n;\\mathbf{w}) = \\frac{1}{1+\\exp\\left(-\\left(y_n \\mathbf{w}^T\\mathbf{x}_n\\right)\\right)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:**\n",
    "\n",
    "Se tienen dos casos, cuando la etiqueta $y_n=1$ se obtiene exactamente $P(y=1|\\mathbf{x}_n;\\mathbf{w})$.    \n",
    "\n",
    "\n",
    "Cuando $y_n=-1$ se debe obtener:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(y=-1|\\mathbf{x}_n;\\mathbf{w}) & = 1 - P(y=1|\\mathbf{x}_n;\\mathbf{w})  \\\\\n",
    "                                & = \\frac{\\exp\\left(-\\left(\\mathbf{w}^T\\mathbf{x}_n\\right)\\right)}{1+\\exp\\left(-\\left(\\mathbf{w}^T\\mathbf{x}_n\\right)\\right)}  \\\\\n",
    "                                & =  \\frac{1}{1+\\exp\\left( \\left(\\mathbf{w}^T\\mathbf{x}_n\\right)\\right)}    \\\\                   \n",
    "                                & =  \\frac{1}{1+\\exp\\left( -\\left( y_n \\mathbf{w}^T\\mathbf{x}_n\\right)\\right)} \\\\\n",
    "                                 & = \\theta(y_n \\mathbf{w}^T\\mathbf{x}_n)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ej3 b)** Mostrar que la log-verosimilitud $l(\\mathbf{y}| \\mathbf{X};\\mathbf{w})$ en el modelo logístico se escribe como $$\n",
    " l(\\mathbf{y}| \\mathbf{X};\\mathbf{w}) = \\sum_{n=1}^{N} \\log \\left( \\theta(y_n \\mathbf{w}^T\\mathbf{x}_n) \\right)\n",
    "$$\n",
    "\n",
    "donde $\\mathbf{X}$ es una matriz de $N \\times (d+1)$ e $\\mathbf{y}$ es un vector que contiene las $N$ etiquetas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:**\n",
    "\n",
    "La probabilidad de obtener las etiquetas $y_1, \\ldots, y_n$ a partir de los $\\mathbf{x}_1, \\ldots ,\\mathbf{x}_n $ es:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(y_1, \\ldots, y_n \\vert \\mathbf{x}_1, \\ldots ,\\mathbf{x}_n) &= \\prod_{n=1}^N P(y=y_n \\vert \\mathbf{x}_n)  \\\\\n",
    "                                                             &=  \\prod_{n=1}^N \\theta(y_n \\mathbf{w}^T\\mathbf{x}_n)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Tomando el logaritmo, se tiene:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "l(\\mathbf{y}| \\mathbf{X};\\mathbf{w}) & = \\log \\left( \\prod_{n=1}^N \\theta(y_n \\mathbf{w}^T\\mathbf{x}_n) \\right)  \\\\\n",
    "                                      & =  \\sum_{n=1}^{N} \\log \\left( \\theta(y_n \\mathbf{w}^T\\mathbf{x}_n) \\right)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ej3 c)** Mostrar que encontrar el $\\mathbf{w}$ que maximiza la log-verosimilitud es equivalente a encontrar el $\\mathbf{w}$ que minimiza la función de costo: \n",
    "\n",
    "$$\n",
    "E_{in}(\\mathbf{w}) = \\frac{1}{N}\\sum_{n=1}^N \\log \\left( 1 + \\exp \\left( -y_n \\mathbf{w}^T\\mathbf{x}_n \\right) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solución:**\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\arg \\max l(\\mathbf{y}| \\mathbf{X};\\mathbf{w}) & = \\arg \\max \\sum_{n=1}^{N} \\log \\left( \\theta(y_n \\mathbf{w}^T\\mathbf{x}_n) \\right) \\\\\n",
    "                                                & = \\arg \\min -\\sum_{n=1}^{N} \\log \\left( \\theta(y_n \\mathbf{w}^T\\mathbf{x}_n) \\right) \\\\ \n",
    "                                                & = \\arg \\min \\sum_{n=1}^{N} \\log \\left( \\frac{1}{\\theta(y_n \\mathbf{w}^T\\mathbf{x}_n)} \\right) \\\\ \n",
    "                                                & = \\arg \\min \\sum_{n=1}^{N} \\log \\left(  1 + \\exp \\left( -y_n \\mathbf{w}^T\\mathbf{x}_n \\right)  \\right) \\\\   \n",
    "                                                & = \\arg \\min N E_{in} \\\\\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ej3 d)** Mostrar que $$\\nabla E_{in}(\\mathbf{w}) = \\frac{1}{N} \\sum_{n=1}^{N} -y_n \\mathbf{x}_n \\theta  \\left( - y_n \\mathbf{w}^T\\mathbf{x_n} \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Respuesta:**    \n",
    "\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
